{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VanillaCGN(nn.Module):\n",
    "    def __init__(self, input_dim, node_dim, n_layers) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.node_dim = node_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.U0 = nn.init.kaiming_normal_(torch.empty(input_dim, node_dim))\n",
    "        self.b0 = nn.Parameter(torch.randn(node_dim))\n",
    "        self.convLayers = nn.ModuleList(\n",
    "            [ConvNetLayer(self.node_dim) for _ in range(self.n_layers)]\n",
    "        )\n",
    "        self.readOutLayer = GraphRegressionReadoutLayer(node_dim=node_dim)\n",
    "\n",
    "    def forward(self, x, adj_mat):\n",
    "        x = x @ self.U0 + self.b0  # self.b0 is broadcasted properly?\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.convLayers[i](x, adj_mat)\n",
    "        x = x.sum(dim=0)  # Check\n",
    "        x = self.readOutLayer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNetLayer(nn.Module):\n",
    "    def __init__(self, node_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.U = nn.Parameter(torch.randn(node_dim, node_dim))\n",
    "\n",
    "    def forward(self, x, adj_mat):\n",
    "        new_x = torch.empty_like(x)\n",
    "        for i in range(x.shape[0]):\n",
    "            deg_i = adj_mat[:, i].sum()\n",
    "            mask_i = adj_mat[:, i] > 0\n",
    "            new_x[i, :] = F.relu(\n",
    "                self.U @ (x[mask_i, :].sum(dim=0)).to(torch.float32) / deg_i\n",
    "            )\n",
    "        return new_x\n",
    "\n",
    "\n",
    "class GraphRegressionReadoutLayer(nn.Module):\n",
    "    def __init__(self, node_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.node_dim = node_dim\n",
    "        self.Q = nn.Parameter(nn.init.kaiming_normal_(torch.empty(node_dim, node_dim)))\n",
    "        self.P = nn.Parameter(nn.init.kaiming_normal_(torch.empty((1, node_dim))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.P @ F.relu(self.Q @ x)).squeeze()\n",
    "\n",
    "def build_adj_mat(edge_index):\n",
    "    \"\"\"Build the adjacency matrix of a graph from edge_index that looks like this: [[1, 4, 5, 6], [2, 3, 3, 5]]\n",
    "    cf. https://huggingface.co/datasets/graphs-datasets/ZINC for more details\"\"\"\n",
    "    n_nodes = max(max(edge_index[0]), max(edge_index[1])) + 1\n",
    "    n_edges = len(edge_index[0])\n",
    "    adj_mat = torch.zeros((n_nodes, n_nodes))\n",
    "    for i in range(n_edges):\n",
    "        adj_mat[edge_index[0][i], edge_index[1][i]] = 1\n",
    "    adj_mat += torch.eye(adj_mat.shape[0])  # Ensures deg_i > 0 and stabilize training\n",
    "    return adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaCGN(input_dim=1, node_dim=10, n_layers=4)\n",
    "y_pred = model(node_feat, adj_mat)\n",
    "\n",
    "# Y = torch.rand((3, 2))\n",
    "# edge_index = [[0, 1, 1, 2],[1, 0, 2, 1]]\n",
    "# adj_mat = build_adj_mat(edge_index)\n",
    "# # for i in range(5):\n",
    "# Y = model(Y, adj_mat)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = torch.tensor([[[0],\n",
    "         [1],\n",
    "         [0],\n",
    "         [0],\n",
    "         [0],\n",
    "         [1],\n",
    "         [0],\n",
    "         [2],\n",
    "         [0],\n",
    "         [1],\n",
    "         [4],\n",
    "         [0],\n",
    "         [0],\n",
    "         [1],\n",
    "         [2],\n",
    "         [0],\n",
    "         [0],\n",
    "         [0],\n",
    "         [0],\n",
    "         [0],\n",
    "         [0],\n",
    "         [0]]])\n",
    "\n",
    "adj_mat = torch.tensor([[[1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 1.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
    "          0., 0., 0., 0., 1.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
    "          0., 0., 0., 1., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
    "          1., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "          1., 1., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          1., 1., 1., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 1., 1., 1., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "          0., 0., 1., 1., 0.],\n",
    "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 1.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConvNetLayer.forward() missing 1 required positional argument: 'adj_mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      3\u001b[0m layer \u001b[38;5;241m=\u001b[39m ConvNetLayer(node_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: ConvNetLayer.forward() missing 1 required positional argument: 'adj_mat'"
     ]
    }
   ],
   "source": [
    "# adj_mat = torch.tensor([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "# X = torch.tensor([[5, 2], [3, 4], [10, 20]], dtype=torch.float32)\n",
    "# layer = ConvNetLayer(node_dim=2)\n",
    "\n",
    "# X = layer(X)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
